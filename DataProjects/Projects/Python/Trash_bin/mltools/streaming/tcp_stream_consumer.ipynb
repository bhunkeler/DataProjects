{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "e3992ac8625e869d8834b8ac3532ba15a3662cc2bf5b306aa84d926baaf2d986"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import socket\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "import socket, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc = SparkContext()\n",
    "spark =  SparkSession.builder.appName('trash_bin').master('local').getOrCreate()\n",
    "ssc = StreamingContext(spark, 10 )\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Read input stream from socket (by default, sockets contain raw strings, which we must then parse in a structured format)\n",
    "\n",
    "HOST = '192.168.0.151'\n",
    "# HOST = '127.0.0.1'\n",
    "PORT = 65433\n",
    "NYSE = True\n",
    "\n",
    "# def getMasterNodeIP():\n",
    "#   from urllib.parse import urlparse\n",
    "#   try:\n",
    "#     url_parsed = urlparse(sc.uiWebUrl)\n",
    "#     ip = url_parsed.netloc.split(':')[0]\n",
    "#     return ip\n",
    "#   except Exception as exp:\n",
    "#     print(exp)\n",
    "#     return None\n",
    "\n",
    "lines = spark.readStream.format(\"socket\").option(\"host\", HOST).option(\"port\", PORT).load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'json_schema' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-d48adc125ee0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m   \u001b[0mspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mreadStream\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;33m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_schema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputPath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'json_schema' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NYSE\n"
     ]
    }
   ],
   "source": [
    "if NYSE:\n",
    "    print('NYSE')\n",
    "    structured_stream = lines.select(\\\n",
    "        split(lines.value, \",\")[0].alias(\"exchange\"),\\\n",
    "        split(lines.value, \",\")[1].alias(\"company\"),\\\n",
    "        split(lines.value, \",\")[2].alias(\"date\"),\\\n",
    "        split(lines.value, \",\")[3].cast('Float').alias(\"price_open\"),\\\n",
    "        split(lines.value, \",\")[4].cast('Float').alias(\"price_high\"),\\\n",
    "        split(lines.value, \",\")[5].cast('Float').alias(\"price_low\"),\\\n",
    "        split(lines.value, \",\")[6].cast('Float').alias(\"price_close\"),\\\n",
    "        split(lines.value, \",\")[7].cast('Int').alias(\"stock_volume\"),\\\n",
    "        split(lines.value, \",\")[8].cast('Float').alias(\"price_adj_close\"),\\\n",
    "        split(lines.value, \",\")[9].cast('Timestamp').alias(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[exchange: string, company: string, date: string, price_open: float, price_high: float, price_low: float, price_close: float, stock_volume: int, price_adj_close: float, timestamp: timestamp]"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "structured_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NYSE == False:\n",
    "    print('trash_bin')\n",
    "    structured_stream = lines.select(\\\n",
    "        split(lines.value, \",\")[0].cast('string').alias(\"canton\"),\\\n",
    "        split(lines.value, \",\")[1].cast('string').alias(\"canton_code\"),\\\n",
    "        split(lines.value, \",\")[2].cast('string').alias(\"city\"),\\\n",
    "        split(lines.value, \",\")[3].cast('string').alias(\"country\"),\\\n",
    "        split(lines.value, \",\")[4].cast('string').alias(\"country_code\"),\\\n",
    "        split(lines.value, \",\")[5].cast('int').alias(\"filling_level\"),\\\n",
    "        split(lines.value, \",\")[6].cast('int').alias(\"id\"),\\\n",
    "        split(lines.value, \",\")[7].cast('double').alias(\"lat\"),\\\n",
    "        split(lines.value, \",\")[8].cast('double').alias(\"lon\"),\\\n",
    "        split(lines.value, \",\")[9].cast('boolean').alias(\"maintenance\"),\\\n",
    "        split(lines.value, \",\")[11].cast('string').alias(\"street\"),\\\n",
    "        split(lines.value, \",\")[12].cast('string').alias(\"timestamp\"),\\\n",
    "        split(lines.value, \",\")[13].cast('boolean').alias(\"update\"),\\\n",
    "        split(lines.value, \",\")[13].cast('boolean').alias(\"level_50\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed_stream = structured_stream.groupBy(window(\"timestamp\", \"4 seconds\", \"4 seconds\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NYSE:\n",
    "    aggregation_stream = windowed_stream.agg(avg(\"price_high\").alias(\"window_price_average\"),\n",
    "                                           sum(\"price_high\").alias(\"price_sum\"), count(\"timestamp\").alias(\"count_prices\"))\n",
    "else: \n",
    "    aggregation_stream = windowed_stream.agg(avg(\"filling_level\").alias(\"window_filling_level_average\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: always use 'complete' outputMode for aggregations and 'append' outputMode to get complete records (the entire stream)\n",
    "\n",
    "streamingETLQuery = aggregation_stream.writeStream.format(\"memory\").queryName(\"aggDF\").outputMode(\"complete\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %sql\n",
    "\n",
    "spark.sql('SELECT window.start, window.end, window_price_average FROM aggDF')\n",
    "# SELECT * from aggDF\n",
    "\n",
    "query_result = spark.sql('SELECT * FROM aggDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "#TODO: complete here the SQL statement required to get all the contents of the stream (use the name assigned earlier)\n",
    "df = spark.sql(\"select * from aggDF\")\n",
    "print(\"Running SQL (initialization may take a while)...\")\n",
    "\n",
    "iter = 1\n",
    "\n",
    "# initialize the historic average with some random value\n",
    "prev_avg = 1.0\n",
    "old_count = 0\n",
    "\n",
    "while True:\n",
    "  \n",
    "  # only start computing once some data has been collected (note that the dataframe is automatically updated by Spark)\n",
    "  if(df.count() != 0):\n",
    "    while(old_count == df.count()):\n",
    "      # don't do anything while there is no new data\n",
    "      continue\n",
    "      \n",
    "    # update the total count in order to be able to use the condition above in the next iteration\n",
    "    old_count = df.count()\n",
    "    \n",
    "    print(\"*********************************************************************************************************\")\n",
    "    print(\"\\nIteration no. \"+ str(iter) + \"\\n\")\n",
    "    print(\"Sample data from streaming dataframe:\\n\")\n",
    "    \n",
    "    # set the time to be the end of the window (just to have a visual indication of time)\n",
    "    df = df.withColumn(\"time\", df.window.end)\n",
    "       \n",
    "    # we sort by time first, because by default there is no ordering in the streaming dataframe and we want to only show the most recent results\n",
    "    # the following allows showing the most recent 5 windows (the False parameter instructs Spark not to truncate the output)\n",
    "    df = df.sort(desc(\"time\"))\n",
    "    df.show(5, False)\n",
    "\n",
    "    print(\"Current number of windows processed in stream: \"+ str(old_count) + \"\\n\")\n",
    "\n",
    "    #TODO: use here the field that denotes the sum of prices per window.\n",
    "    # In order to get the overall historic sum, you must add all the prices per window (use a new aggregate to do this)\n",
    "    val = df.select('price_sum').agg(sum('price_sum')).collect()[0][0]\n",
    "\n",
    "    #TODO: the same for the counts of all items with prices\n",
    "    counts = df.select('count_prices').agg(sum('count_prices')).collect()[0][0]\n",
    "    if(counts != None and counts != 0):\n",
    "      # update the historic average taking into account the new aggregated values\n",
    "      historic_avg = val/counts\n",
    "     \n",
    "   \n",
    "    # the following lines just handle pretty-printing the output...\n",
    "    for indent in range(iter - 1):\n",
    "      sys.stdout.write(\"|****|\")\n",
    "    print (\"|----|  Average price before current window: \" + str(prev_avg))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    #TODO: take the most recent window average - use the field name you assigned for the average price per window\n",
    "    # since we already ordered by time, you can simply take the first element of the result\n",
    "    current_avg = df.select(\"window_price_average\").take(1)[0][0]\n",
    "    \n",
    "    # the following lines just handle pretty-printing the output...\n",
    "    for indent in range(iter - 1):\n",
    "      sys.stdout.write(\"      \") \n",
    "    sys.stdout.write(\"|****| \" + \" Current window average price: \" + str(current_avg))\n",
    "    print\n",
    "\n",
    "    # compute the percentage change from the historic average up until this window (ignoring the first iteration, where the historic average is a random number)\n",
    "    if((math.fabs(current_avg - prev_avg)/prev_avg) > 0.3 and iter > 1):\n",
    "        print(\"\\n\\t!!! ANOMALY DETECTED: price fluctuated by \"+ str(float(\"{0:.2f}\".format(math.fabs(current_avg - prev_avg)/prev_avg)) * 100) + \" % !!!\\n\\n\\n\")\n",
    "        \n",
    "    prev_avg = historic_avg\n",
    "    iter += 1"
   ]
  }
 ]
}
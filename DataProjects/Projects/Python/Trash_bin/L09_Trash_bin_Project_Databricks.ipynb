{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "e3992ac8625e869d8834b8ac3532ba15a3662cc2bf5b306aa84d926baaf2d986"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# THIS IS NOT YET IMPLEMENTED"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Trash bin project\n",
    "\n",
    "Any city in the world has trash-bins, which fill up over time. Each of the trash bins needs to be emptied from time to time.   \n",
    "Each workforce in a city work on a certain schedule and therefore on predefined routes to empty trash-bins. The assumption is, that a route always   \n",
    "follows the same pattern. A person of the workforce will start it's route and approach trash-bins, which have a filling level of 10%, 20% etc.   \n",
    "Some of the trash-bins are approached and are almost empty. In order to reduce the effort resp. time consumption an optimization should be considered.  \n",
    "\n",
    "The project idea at hand is to equip each and every trash-bin with an IoT component, which communicates with a central server. The IoT compnent  \n",
    "should be able to send a timestamp, location- and filling information (e.g. 10, 20, ..., 100%) etc., but also hardware status information about the   \n",
    "component itself. \n",
    "\n",
    "The received information must be processed via big data. A threshold needs to be defined (filling level e.g. 50%), where a trash-bin needs attention.   \n",
    "The appliaction should monitor the filling levels and propose an optimale route (leeds to an optimization) based on the `travelling salesman problem`.   \n",
    "A simulated annealing optimization algorithm or an OR - Tools approach should be used to resolve the optimal route of a Garbageman.   \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Context Diagram \n",
    "\n",
    "The context diagram shows the main building blocks of the application at hand.\n",
    "\n",
    "<img src=\"images/Context_1.jpg\" alt=\"Context Diagram\" width=\"800\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Scope\n",
    "\n",
    "- Simulated data (JSON) for Switzerland and Winterthur (any number of trash bin's is ok, but 26 have been used for each)\n",
    "- Considere filling level of trash-bins and HW maintenance in optimal route\n",
    "- Optimal Route via Travelling salesman problem \n",
    "- Visualization in gmaps with markers and optimal route\n",
    "- TCP Streaming consumer/producer for Data Analytics\n",
    "- TCP Client/Server for Route Analytics\n",
    "\n",
    "Out of scope  \n",
    "- Historic data \n",
    "- Predictive failure detection of HW Module\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Key Performance Indicator (KPI)\n",
    "\n",
    "- None of the trash bins should be full (consider filling level > 50%)\n",
    "- hardware failure should also be considered in a route\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Building Blocks of IoT Device Hardware\n",
    "\n",
    "In many IoT products, the “thing” is fully integrated into the smart device. For example, think of products like an autonomous trash bin. These products control and monitor themselves. In this case, your product includes all four building blocks in a single package as shown below.\n",
    "\n",
    "<img src=\"images/iot_building_blocks.jpg\" alt=\"Building Blocks\" width=\"500\">\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Building Block 1: Thing\n",
    "I define “thing” as the asset that you want to control or monitor.\n",
    "\n",
    "In many IoT products, the “thing” is fully integrated into the smart device. For example, think of products like an autonomous trash bin. These products control and monitor themselves. In this case, your product includes all four building blocks in a single package as shown below.\n",
    "\n",
    "#### Building Block 2: Data Acquisition Module\n",
    "The data acquisition module focuses on acquiring physical signals from the “thing” and converting them into digital signals that can be   \n",
    "manipulated by a computer.\n",
    "\n",
    "This is the hardware component that includes all the sensors acquiring real-world signals such as temperature, motion, light, vibration,   \n",
    "etc. The type and number of sensors you need depend on your application.\n",
    "\n",
    "#### Building Block 3: Data Processing Module\n",
    "The third building block of the device is the data processing module. This is the “computer” that processes the data, performs local analytics,   \n",
    "stores data locally, and performs any other computing operations at the edge.\n",
    "\n",
    "#### Building Block 4: Communications Module\n",
    "The last building block of your device’s hardware is the communications module. This is the circuitry that enables communications with your Cloud Platform,   \n",
    "and with 3rd party systems either locally or in the Cloud."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Sensor Data / Distance Matrix\n",
    "\n",
    "### Sensor Data\n",
    "A Streaming service, as far as I know, does not exist for such kind of data. Therefore, synthetic data needs to be created to simulate the project.   \n",
    "The data will be structured  in a JSON format as depicted below. A CSV Format could also be considered. An entry contains various fields, but the   \n",
    "main fields, which will be processed in the appliaction at hand are `ID, Name, Point (lat and lon - coordinates)`, `filling level` and the   \n",
    "`maintenance flag`.  \n",
    "\n",
    "<img src=\"images/image001.jpg\" alt=\"JSON Structure\" width=\"400\">\n",
    "\n",
    "### Sensor Data for Streaming\n",
    "The data structure sligthly varies in that sense that while streaming the data, each location is applied as a json string in a separate row. \n",
    "\n",
    "**Excerpt:**  \n",
    "{\"id\": 2,\"street\": \"Street 2\",\"city\": \"Bern\",\"country\": \"Switzerland\",\"country_code\": \"CH\",\"canton\": \"Bern\",\"canton_code\": \"BE\",\"lat\": 46.948384,\"lon\": 7.439946,\"filling_level\": 22,\"time\": 1610241300,\"maintenance\": false, \"update\": false}   \n",
    "{\"id\": 3,\"street\": \"Street 3\",\"city\": \"Luzern\",\"country\": \"Switzerland\",\"country_code\": \"CH\",\"canton\": \"Luzern\",\"canton_code\": \"LU\",\"lat\": 47.055245,\"lon\": 8.305225,\"filling_level\": 17,\"time\": 1610241300,\"maintenance\": false, \"update\": false}\n",
    "\n",
    "### Distance Matrix\n",
    "`EPSG Geodetic Parameter` Dataset (also EPSG registry) is a public registry of geodetic datums, spatial reference systems, Earth ellipsoids,   \n",
    "coordinate transformations and related units of measurement. Each entity is assigned an EPSG code between 1024-32767. The EPSG is the basis for   \n",
    "the distance calculation in the project. Finding the optimal way requires aspects from `garph theaory` as `vertices` and `edges`. A distance Matrix   \n",
    "is required to define the weights of the edges between vertices. \n",
    "\n",
    "Calculating the Distance Matrix requires the coordinates (lat, lon) of each location. Since the project at hand requires the distance between two points   \n",
    "in respect of the avilable streets rather then the distance by air, I used googles Directions- and Distance Matrix API. Distance Matrix calculation is   \n",
    "part of the `Location_MAP class`.   \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Package Builder\n",
    "The databricks environment has the little drawback, that additional source code required in a project needs to be installed as python packages.   \n",
    "So a trash-bin Package `(trashbin-0.0.5-py3-none-any.whl)`, considering all dependencies has been created for that purpose.  \n",
    "\n",
    "Python `Package_builder` Project is available here: [Package_builder]()\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## TCP Services for Simulation\n",
    "\n",
    "### TCP Client/Server for Simulation\n",
    "As mentioned at the beginning there is no streaming service available providing the required data. Therefore, I implemented my own `TCP_Server`   \n",
    "and a respective `TCP_Client`, to simulate the streaming service. The TCP_Server is not highely sophisticated, it's only `single threaded`.   \n",
    "We save the `multithreaded server` for latter on. \n",
    "\n",
    "The `TCP_Server` contains mainly the following methods: \n",
    "\n",
    "- handler - handles all incomming requests from the `TCP_Client` and does a triage\n",
    "- receive - the TCP_Server `receives` changed trash bin data and `updates` them. This is require, when a garbageman emptied a trash bin\n",
    "- submit  - the TCP_client `requests` actual data of each trash bin from the TCP_Server \n",
    "- random  - `generate random values` on the server to get the feeling the trash bins are filled over time\n",
    "- acknowledge - acknowledge is only to return the respective state from server to client \n",
    "\n",
    "The `TCP_Client` contains mainly the following methods: \n",
    "\n",
    "- initalize - `initialize` the `server side data set`\n",
    "- request_sensor_data  - `requests sensor data` from TCP_Server\n",
    "- update_sensor_data   - `sends changed sensor data` to be updated on the server \n",
    "- assign_random_values - request to `assign random values` to the `filling_level`\n",
    "\n",
    "### TCP Streaming Consumer/Producer for Simulation\n",
    "\n",
    "TCP_Server and TCP_Client located in mltools.streaming\n",
    "\n",
    "The `TCP_Streaming_Server` contains the following method: \n",
    "- send() - `send` data via TCP from a file\n",
    "\n",
    "The `TCP_Streaming_Consumer` contains the following method: \n",
    "- receive() - `receive` consumes the sent streaming data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Travelling Salesman Problem (TSP)\n",
    "The most famous routing problem is the `Traveling Salesman Problem (TSP)`:   \n",
    "find the shortest route for a salesman who needs to visit customers at different locations and return to the starting point.   \n",
    "A TSP can be represented by a graph, in which the nodes correspond to the locations, and the edges (or arcs) denote direct   \n",
    "travel between locations. The distance between any two locations is given by the weight (distance) next to the edge joining them.\n",
    "\n",
    "#### Simulated annealing \n",
    "Simulated annealing is a probabilistic optimization scheme which guarantees convergence to the global minimum given sufficient run time.  \n",
    "It’s loosely based on the idea of a metallurgical annealing in which a metal is heated beyond its critical temperature and cooled according   \n",
    "to a specific schedule until it reaches its minimum energy state. This controlled cooling regiment results in unique material properties useful   \n",
    "for specific applications.\n",
    "\n",
    "At it’s core, simulated annealing is based on equation (1) which represents the probability of jumping to the next energy level. Within the context   \n",
    "of simulated annealing, energy level is simply the current value of whatever function that’s being optimized.\n",
    "\n",
    "$$P(e_c, e_n,T) = exp(-\\Delta{E} / T)$$\n",
    "\n",
    "$e_c$       - energy at current state  \n",
    "$e_n$       - energy at proposed neighbor site  \n",
    "$\\Delta{E}$ - change in energy between current state and proposed neighbor state  \n",
    "$T$         - current system temperature  \n",
    "\n",
    "The main loop for simulated annealing consists of generating neighbor candidates which are just potential solutions which are then randomly accepted   \n",
    "based on an ever increasingly more stringent threshold\n",
    "\n",
    "#### Vehicle routing problems  \n",
    "`Vehicle routing problems` are inherently intractable: the length of time it takes to solve them grows exponentially with the size of the problem.   \n",
    "For sufficiently large problems, it could take OR-tools (or any other routing software) years to find the optimal solution. As a result, OR-Tools   \n",
    "sometimes returns solutions that are good, but not optimal.\n",
    "\n",
    "`OR-Tools` is open source software for combinatorial optimization, which seeks to find the best solution to a problem out of a    \n",
    "very large set of possible solutions. \n",
    "\n",
    "**Note:**    \n",
    "We should add that there are other solvers, such as Concorde, dedicated to solving very large TSPs to optimality, which surpass `OR-Tools` or   \n",
    "`Simulated Annealing` in that area. However, OR-Tools provides a better platform for solving more general routing problems that contain   \n",
    "constraints beyond those of a pure TSP.  \n",
    "Both approaches have been implemented and are available in the sources, but considering calculation time the OR-Tool has been applied.    \n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Vizualization \n",
    "\n",
    "### Googlemaps / gmaps\n",
    "`Google Maps` is a web mapping service developed by Google. It offers satellite imagery, aerial photography, street maps, 360° interactive   \n",
    "panoramic views of streets, real-time traffic conditions, and route planning for travelling by foot, car, bicycle, air and public transportation. \n",
    "\n",
    "`gmaps` is a plugin for Jupyter for embedding `Google Maps` in notebooks. It is designed as a data visualization tool. In the current project  \n",
    "`googlemaps` has been used to visualize the `maps`, while `gmaps` has been applied to place `markers` and `directions`.\n",
    "\n",
    "### Analytics Dashboard\n",
    "\n",
    "- Filling level per location and respective time window (hour)\n",
    "- HW - Failure over time\n",
    "- Maximum filling level and location in a given time window \n",
    "- Garbage volume of all trash bins when filling level > 50% -> Assumption of car size required (small / medium / large)\n",
    "- Average amout of trash per time indow (hour)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Code Section\n",
    "### Preliminaries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_on_databricks = False\n",
    "\n",
    "# Distinguish between Switzerland or Winterthur\n",
    "country = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Spark libraries \n",
    "import pyspark\n",
    "import socket\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "import socket, time\n",
    "\n",
    "# project libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "# Trash_bin main\n",
    "from mltools.trash_bin import TrashBin\n",
    "from mltools.config import Configuration\n",
    "\n",
    "# Maps\n",
    "import gmaps\n",
    "import googlemaps\n",
    "from mltools.map import Map\n",
    "from mltools.location_map import Location_Map\n",
    "\n",
    "# Simulated annealing\n",
    "from mltools.annealing.nodes_generator import Node_Generator\n",
    "from mltools.annealing.simulated_annealing import Simulated_Annealing\n",
    "\n",
    "# Route analytis\n",
    "from mltools.tsp.route_analytics import Route_Analytics \n",
    "from mltools.tsp.travelling_salesman import TSP\n",
    "from ortools.constraint_solver import routing_enums_pb2\n",
    "from ortools.constraint_solver import pywrapcp\n",
    "\n",
    "from plotnine import *\n",
    "\n",
    "if ((run_local == False) and (run_on_local_spark == False) and (run_on_databricks == True)):\n",
    "    !pip install googlemaps\n",
    "    !pip install gmaps\n",
    "    # !!pip install bunch\n",
    "    # !pip install GDAL\n",
    "    # !/databricks/python3/bin/python3.7 -m pip install --upgrade pip\n",
    "    # !pip install jupyter\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "source": [
    "### Map, location and sensor data pre - processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/FileStore/tables/trash_bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if country:\n",
    "  location_data = spark.read.load(\"/FileStore/tables/trash_bin/sz_locations.csv\", format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "  dist_matrix   = spark.read.load(\"/FileStore/tables/trash_bin/sz_dist_matrix.csv\", format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "  sensor_data   = spark.read.option(\"multiline\", \"true\").json(\"/FileStore/tables/trash_bin/sz_sensor_spark.json\")\n",
    "  json_schema = sensor_data.schema\n",
    "  sensor_data.printSchema()\n",
    "else: \n",
    "  location_data = spark.read.load(\"/FileStore/tables/trash_bin/wt_locations.csv\", format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "  dist_matrix   = spark.read.load(\"/FileStore/tables/trash_bin/wt_dist_matrix.csv\", format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "  sensor_data   = spark.read.option(\"multiline\", \"true\").json(\"/FileStore/tables/trash_bin/wt_sensor_spark.json\")\n",
    "  json_schema = sensor_data.schema\n",
    "  sensor_data.printSchema()\n",
    "    \n",
    "# convert spark dataframe to pandas dataframe  \n",
    "location_data = location_data.select(\"*\").toPandas()\n",
    "dist_matrix   = dist_matrix.select(\"*\").toPandas()\n",
    "sensor_data   = sensor_data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['id', 'street', 'city', 'country', 'country_code', 'canton', 'canton_code', 'point', 'lat', 'lon', 'filling_level', 'timestamp', 'maintenance']\n",
    "\n",
    "sensor_data = sensor_data.reindex(columns = column_names)\n",
    "sensor_data['level_50'] = np.where(sensor_data['filling_level'] >= 50, True, False)\n",
    "sensor_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sensor_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean current environment\n",
    "'''\n",
    "dbutils.fs.rm(\"/FileStore/tables/trash_bin/sz_locations.csv\")\n",
    "dbutils.fs.rm(\"/FileStore/tables/trash_bin/sz_dist_matrix.csv\")\n",
    "dbutils.fs.rm(\"/FileStore/tables/trash_bin/sz_sensor.json\")\n",
    "dbutils.fs.rm(\"/FileStore/tables/trash_bin/sz_sensor_spark.json\")\n",
    "\n",
    "dbutils.fs.rm(\"/FileStore/tables/trash_bin/wt_locations.csv\")\n",
    "dbutils.fs.rm(\"/FileStore/tables/trash_bin/wt_dist_matrix.csv\")\n",
    "dbutils.fs.rm(\"/FileStore/tables/trash_bin/wt_sensor.json\")\n",
    "dbutils.fs.rm(\"/FileStore/tables/trash_bin/wt_sensor_spark.json\")\n",
    "'''\n",
    "\n",
    "dbutils.fs.rm(\"/FileStore/tables/trash_bin/sz_sensor_sp.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = TrashBin()\n",
    "map = Location_Map(tb.get_config('trash_bin'), location_data)\n",
    "config = tb.get_config('tsp')\n"
   ]
  },
  {
   "source": [
    "%md\n",
    "### Sensor data \n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sensor = tb.load_sensor_data(sensor_data)\n",
    "df_sensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill level >= 50%\n",
    "sensors = np.where(df_sensor['filling_level'] >= 50)\n",
    "\n",
    "# maintenance == True\n",
    "maintenance = np.where(df_sensor['maintenance'] == True)\n",
    "nodes = set(np.concatenate([sensors[0], maintenance[0]]))\n",
    "nodes = list(nodes)  # convert to list again\n",
    "print(nodes)"
   ]
  },
  {
   "source": [
    "%md\n",
    "### Evaluate shortest distance (Travelling Salesman problem)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsp = TSP()\n",
    "data, ref = tsp.create_data_model(dist_matrix, nodes, prod = True)\n",
    "\n",
    "route_analytics = Route_Analytics()\n",
    "manager, routing, solution = route_analytics.estimate_route(data)\n",
    "route, route_distance = tsp.solution(manager, routing, solution)\n",
    "route_tsp = route_analytics.ref_map(route, ref)\n",
    "\n",
    "print('Route Distance: {} [m]'.format(route_distance))\n",
    "print('Route IDs: {}'.format(route))"
   ]
  },
  {
   "source": [
    "## Map\n",
    "### Prepare markers and directions\n",
    "Markers are location points, which the garbage man needs to approach. Markers come in two flavours \"filling level\" and \"maintenance\".   \n",
    "The location of a trash bin will indicated with a marker, as soon as he exceeds the 50% filling level. Another marker is applied for   \n",
    "maintenance reasons. Hardware tends to fail over time. Therefore, we also consider the hw maintenance flag, which needs to be considered   \n",
    "as alocation point on a route.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSP\n",
    "print(route_tsp)\n",
    "locations_tsp = route_tsp\n",
    "\n",
    "location_markers_tsp = []\n",
    "\n",
    "for location in route_tsp: \n",
    "    location_markers_tsp.append(map.location_point(location))\n",
    "\n",
    "print(location_markers_tsp)"
   ]
  },
  {
   "source": [
    "### Initialize the Base map"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = map.plot_base_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(route_tsp)\n",
    "fig = map.add_marker(fig, location_markers_tsp)\n",
    "\n",
    "length = len(route_tsp)\n",
    "for i, location in enumerate(location_markers_tsp):\n",
    "    if i < (length - 1):\n",
    "        fig = map.add_direction(fig, location, location_markers_tsp[i + 1])\n",
    "\n",
    "fig"
   ]
  },
  {
   "source": [
    "### General purpose Methods"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMasterNodeIP():\n",
    "    '''\n",
    "    get the IP from the master node\n",
    "    '''\n",
    "    try:\n",
    "        \n",
    "        url_parsed = urlparse(sc.uiWebUrl)\n",
    "        ip = url_parsed.netloc.split(':')[0]\n",
    "        return ip\n",
    "    \n",
    "    except Exception as exp:\n",
    "        print(exp)\n",
    "        return None"
   ]
  },
  {
   "source": [
    "## Data Stream Producer\n",
    "\n",
    "This notebook provides some basic functionality which one needs to let a stream run so that the Spark Streaming Engine can process it. The stream is produced from a file with trash bin information. In order to simulate streaming data, the producer reads this file continuously and adds a timestamp. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def send(inputFile):\n",
    "  \n",
    "  ifile  = open(inputFile, 'r') \n",
    "  i = 0\n",
    "  \n",
    "        \n",
    "  while True:\n",
    "    client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    client_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
    "    client_socket.bind((getMasterNodeIP(), portNumber))\n",
    "    client_socket.listen(10)\n",
    "    conn, addr = client_socket.accept()\n",
    "\n",
    "    try:  \n",
    "      while True:\n",
    "        start = time.time()\n",
    "        ifile  = open(inputFile, 'r') \n",
    "        for row in ifile.readlines() :\n",
    "\n",
    "          print(\"sending: \"+ row.rstrip() + \",\" + str(datetime.now()))\n",
    "          message = row.rstrip() + ',' + str(datetime.now()) + \"\\n\"\n",
    "          message = message.encode()\n",
    "          conn.send(message)\n",
    "\n",
    "          # send data every 100 ms\n",
    "          time.sleep(0.1)\n",
    "                   \n",
    "    except Exception as e:\n",
    "        if e.errno == 32:\n",
    "          # Broken Pipe Exception\n",
    "          print(\"Socket was closed. Re-initializing\")\n",
    "        else:\n",
    "          print(str(e))\n",
    "        conn.close()\n",
    "        client_socket.close()\n",
    "        continue\n",
    "    finally:\n",
    "        conn.close()\n",
    "        client_socket.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "send(\"/dbfs/FileStore/tables/sensor_data.csv\")"
   ]
  },
  {
   "source": [
    "## Consumer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read input stream from socket (by default, sockets contain raw strings, which we must then parse in a structured format)\n",
    "lines = spark.readStream.format(\"socket\")\\\n",
    "  .option(\"host\", getMasterNodeIP())\\\n",
    "  .option(\"port\", portNumber)\\\n",
    "  .load()\n",
    "\n",
    "# Split the lines by comma (and assign each resulting column a proper name to reflect its contents, using the alias function)\n",
    "# Note that wherever needed, we must also cast the resulting column to its proper type (e.g. Float for prices, instead of the default String)\n",
    "\n",
    "separator = \",\"\n",
    "\n",
    "structuredStream = lines.select(\\\n",
    "  split(lines.value, separator)[0].cast('Int').alias(\"id\"),\\\n",
    "  split(lines.value, separator)[1].alias(\"street\"),\\\n",
    "  split(lines.value, separator)[2].alias(\"street_no\"),\\\n",
    "  split(lines.value, separator)[3].cast('Int').alias(\"x_coord\"),\\\n",
    "  split(lines.value, separator)[4].cast('Int').alias(\"y_coord\"),\\\n",
    "  split(lines.value, separator)[5].cast('Float').alias(\"Ion\"),\\\n",
    "  split(lines.value, separator)[6].cast('Float').alias(\"lat\"),\\\n",
    "  split(lines.value, separator)[7].cast('Int').alias(\"filling_level\"),\\\n",
    "  split(lines.value, separator)[8].cast('Timestamp').alias(\"timestamp\"),\\\n",
    "  split(lines.value, separator)[9].cast('Bool').alias(\"maintenance\"))\n",
    "\n"
   ]
  }
 ]
}